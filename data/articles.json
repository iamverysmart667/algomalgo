{
  "intro.md": {
    "content": "# Introduction to Algorithms\n\nAn **algorithm** is a procedure that takes in input, follows a certain set of steps, and then produces an output. Oftentimes, the algorithm defines a desired relationship between the input and output. For example, if the problem that we are trying to solve is sorting a hand of cards, the problem might be defined as follows:\n\n> **Problem:** Sort the input\\\n> **Input:** A set of 5 cards\\\n> **Output:** The set of 5 input cards, sorted\\\n> **Procedure:** Up to the designer of the algorithm\n\nThis last part is very important, it's the meat and substance of the algorithm. And, as an algorithm designer, you can do whatever you want to produce the desired output! Think about some ways you could sort 5 cards in your hand, and then click below to see some more ideas.\n\n### Algorithm Ideas\n\n> 1. We could simply toss them up in the air and pick them up again. Maybe they'll be sorted. If not, we can try it again and again until it works (spoiler: this is a bad algorithm).\n>\n> 2. We can sort them one at a time, left to right. Let's say our hand looks like {2, 4, 1, 9, 8}. Well, 2 and 4 are already sorted. But then we have a 1. That should go before 4, and it should go before 2. Now we have {1, 2, 4, 9, 8}. 9 is in the right spot because its higher than the card to its left, 4. But 8 is wrong because it's smaller than 9, so we'll just put it before 9. Now, we have {1, 2, 4, 8, 9}, and we're done. This is called [insertion sort][1].\n>\n> 3. We can sort them two at a time, left to right. So, our hand is again {2, 4, 1, 9, 8}. 2 and 4 are good. 4 and 1 need to be swapped, so now we have {2, 1, 4, 9, 8}. 4 and 9 are good. 9 and 8 should be swapped, so now we have {2, 1, 4, 8, 9}. We have to start at the beginning again, but that's no problem. We look at the first pair and see that 2 and 1 need to switch, so now we have {1, 2, 4, 8, 9}, and we're done. This is called [bubble sort][2].\n>\n> 4. There are a million ways to sort this hand of cards! Some are great, most are terrible. It's up to you as the algorithm designer to make a great one.\n\nThe study of algorithms involves finding the best ways to achieve the desired output given an input and a goal. Learning about algorithms is essential to being a successful computer programmer, and understanding them can help give you an idea of how computers work. So, if you'd like to learn to code, it's absolutely essential to learn about algorithms.\n\n## Algorithms and computers\n\nEven though algorithms existed before the modern computer, they lie at the heart of computing and technology. Everything you've ever done on any piece of technology relies on algorithms because they tell technology what to do. Algorithms are responsible for your ability to surf the web at tolerable speeds. Imagine that you're visiting a website, and that website has a lot of unsorted content to show you. If it randomly picked a content order every time you visited it, and threw that order away and tried again if it wasn't correct, you'd be waiting for minutes, hours, or even days before your web page loaded!\n\nStudying computer science and computer programming always involves algorithms because the study of algorithms teaches you to think like a machine so that you can program them in the best way possible. If you'd like to learn how to write applications, make websites, or do data analysis, you need to know about algorithms so that your code will run fast and run correctly.\n\nOn the theoretical side, many of the simpler algorithms have long since been discovered and heavily studied, but there are many areas left to research. For example, in theoretical computer science, a lingering question is whether *P = NP*, or in other words, \"Are problems that can be quickly verified by a computer able to be quickly solved by a computer?\" Currently, we don't think so. But if it turned out to be true, then computing and technology would experience an enormous speed increase that we would all benefit from. However, this would also mean that modern *cryptography* is not safe and any hacker could easily crack codes to any system in the world!\n\nAs computing grew, applications of computing grew along with it. In order to perform the algorithms that would enable those applications, computer scientists needed a way to represent and store that data. If we wanted to input a set of cards into a computer program, how would we store that data? How would we feed it into the algorithm? Early on, it was good enough to simply represent data as computer bits (zeroes and ones). However, that method could never last, it was too difficult and time-consuming.\n\n[Data structures][3] were the answer. Their invention and research is paralleled by, and is often taught alongside, algorithms. The card sorting algorithm, for example, could take in an [array][4] of numbers to represent cards. More data structures were invented over time, and they allowed algorithm design to progress with them. With these in place, it became much easier to reason about, develop, and research algorithms.\n\n## Properties of Algorithms\n\nAlgorithms have 3 main properties that are important to remember during their design and analysis.\n\n> Algorithm Properties:\n>\n> 1. Time complexity. This is the time an algorithm takes to complete, and it is often given using [big O notation][5] with its input as the independent variable. For example, if we want to search a card in the sorted n cards, we can do in logarithmic time, and the time complexity would be *O(log(n))*.\n> 2. Space complexity. This is the space (in computer memory) the algorithm uses during its execution. Again, if we're sorting nn cards, and we need to make an extra array of size nn for each card to do so, the space complexity would be *O(log(n^2))*.\n> 3. Correctness. An algorithm is correct if and only if, for every input, it halts and outputs the correct output. Contrary to what you might think, algorithms that are not correct are sometimes useful. For example, partially correct algorithms only guarantee that if the algorithm halts, the answer will be correct.\n\n## Designing an Algorithm \n\n<br>\n\nWhen designing an algorithm, it is important to remember that computers are not infinitely fast and that computer memory is not infinitely large. That's why we make algorithms, after all. So, maybe you're designing an algorithm for a computer that is super fast but doesn't have much memory. Maybe you'll make some concessions on the computational requirements so that you can save memory.\n\nBut even if you never had to worry about speed or space, you still need to design a good algorithm. Why? You need to design a good algorithm because you need to know that the algorithm will do what you want it to do and that it will stop once it's done. You need to know that it will be correct.\n\n### Efficacy\n\nThe **efficacy** of the algorithm you're designing comes down to **time complexity** and **space complexity**. In an ideal world, the algorithm is efficient in both ways, but there is sometimes a tradeoff between them. It is up to the designer to weigh their needs appropriately in order to strike a balance.\n\nIt is also up to the designer to make a good algorithm. Doing so requires an understanding of algorithms as well as an understanding of existing algorithms to guide your design process. Otherwise, they might find themselves with a bad algorithm.\n\nTwo algorithms that do the same exact thing in different ways could have enormous differences in efficacy. In sorting, for example, [bubble sort][2] requires *O(n)* space during its execution. [Quick sort][6], on the other hand, requires *O(nlg(n))* space. What does that mean for the programmer using those algorithms? Let's assume for simplicity that the input is just 1KB of data (or 8000 bits). Quicksort will require lg(8000) times more space, or almost 13 times more space than bubble sort. Scale that up to inputs of 1GB or even 1TB, and this difference becomes very noticeable and very inefficient.\n\nHowever, it's worth noting that quicksort runs faster than bubble sort by the same factor. Again, it's a tradeoff, and it's up to the designer to understand the tradeoffs and to optimize their design for their needs.\n\n[1]: <https://empty_page1 \"Insertion Sort Info\">\n[2]: <https://empty_page2 \"Bubble Sort Info\">\n[3]: <https://empty_page3 \"Data Structures Info\">\n[4]: <https://empty_page4 \"Arrays Info\">\n[5]: <https://empty_page5 \"Big O Notation Info\">\n[6]: <https://empty_page6 \"Quick Sort Info\">\n\n",
    "left": "Introduction",
    "right": "Introduction to Algorithms",
    "name": "intro.md",
    "state": false
  },
  "time_and_space.md": {
    "content": "# Time and Space Complexity\n\nThere are more than one way to solve a problem. We need to learn how to compare the performance different algorithms and choose the best one to solve a particular problem. While analyzing an algorithm, we mostly consider time complexity and space complexity. Time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the input. Similarly, Space complexity of an algorithm quantifies the amount of space or memory taken by an algorithm to run as a function of the length of the input.\n\nTime and space complexity depends on lots of things like hardware, operating system, processors, etc. However, we don't consider any of these factors while analyzing the algorithm. We will only consider the execution time of an algorithm.\n\nLets start with a simple example. Suppose you are given an array *A* and an integer *x*  and you have to find if *x* exists in array *A*.\n\nSimple solution to this problem is traverse the whole array *A* and check if the any element is equal to *x*.\n\n    for (int i = 0; i < A.size(); i++)\n        if (A[i] == x)\n            return true;\n    return false;\n\nEach of the operation in computer take approximately constant time. Let each operation takes *c* time. The number of lines of code executed is actually depends on the value of *x*. During analyses of algorithm, mostly we will consider worst case scenario, i.e., when *x* is not present in the array *A*. In the worst case, the **if** condition will run *N* times where *N* is the length of the array *A*. So in the worst case, total execution time will be *(N * c + c)*. *N * c* for the **if** condition and *c* for the **return** statement ( ignoring some operations like assignment of *i*).\n\nAs we can see that the total time depends on the length of the array *A*. If the length of the array will increase the time of execution will also increase.\n\n**Order of growth** is how the time of execution depends on the length of the input. In the above example, we can clearly see that the time of execution is linearly depends on the length of the array. Order of growth will help us to compute the running time with ease. We will ignore the lower order terms, since the lower order terms are relatively insignificant for large input. We use different notation to describe limiting behavior of a function.\n\n***O*-notation**:\nTo denote asymptotic upper bound, we use *O*-notation. For a given function *g(n)*, we denote by *O(g(n))* (pronounced “big-oh of g of n”) the set of functions: *O(g(n))* = \n { *f(n)* : there exist positive constants *c* and *n0* such that *0 <= f(n) <= c * g(n)* for all *n >= n0* }\n \n***&Omega;*-notation**:\nTo denote asymptotic lower bound, we use *&Omega;*-notation. For a given function *g(n)*, we denote by *&Omega;(g(n))* (pronounced “big-omega of g of n”) the set of functions: *&Omega;(g(n))* = \n { *f(n)* : there exist positive constants *c* and *n0* such that *0 <= c * g(n) <= f(n)* for all *n >= n0* }\n\n***&Theta;*-notation**:\nTo denote asymptotic tight bound, we use *&Theta;*-notation. For a given function *g(n)*, we denote by *&Theta;(g(n))* (pronounced “big-theta of g of n”) the set of functions: *&Theta;(g(n))* = \n { *f(n)* : there exist positive constants *c1*, *c2* and *n0* such that *0 <= c1 * g(n) <= f(n) <= c2 * g(n)* for all *n > n0* }\n\n ![notation](notation.jpeg)\n\nTime complexity notations\n\nWhile analysing an algorithm, we mostly consider *O*-notation because it will give us an upper limit of the execution time i.e. the execution time in the worst case.\n\nTo compute *O*-notation we will ignore the lower order terms, since the lower order terms are relatively insignificant for large input.\nLet f(N) = 2 * N^2 + 3 * N + 5, *O*(f(N)) = *O*(2 * N^2 + 3 * N + 5) = *O*(N^2)\n\nLet's consider some examples:\n\n    int count = 0;\n    for (int i = 0; i < N; i++) \n        for (int j = 0; j < i; j++)\n            count++;\n\nLet's see how many times **count++** will run.\n\nWhen *i* = 0, it will run 0 times. \\\nWhen *i* = 1, it will run 1 times. \\\nWhen *i* = 2, it will run 2 times and so on.\n\nTotal number of times **count++** will run is 0 + 1 + 2 + ... +(N - 1) = N * (N - 1) / 2. So the time complexity will be *O*(N^2).\n\n    int count = 0;\n    for (int i = N; i > 0; i /= 2)\n        for (int j = 0; j < i; j++)\n            count++;\n\nThis is a tricky case. In the first look, it seems like the complexity is *O*(N*logN). N for the j's loop and logN for i's loop. But its wrong. Lets see why.\n\nThink about how many times **count++** will run.\n\nWhen *i* = N, it will run N times. \\\nWhen *i* = N/2, it will run N/2 times. \\\nWhen *i* = N/4, it will run N/4 times and so on.\n\nTotal number of times **count++** will run is N + N/2 + N/4 + ... + 1 = 2 * N. So the time complexity will be *O*(N).\n\nThe table below is to help you understand the growth of several common time complexities, and thus help you judge if your algorithm is fast enough for a given input.\n\n|**Length of Input (N)** | **Worst Accepted Algorithm**|\n|---|---|\n|<= [10..11]| *O*(N!), *O*(N^6)|\n|<= [15..18]| *O*(2^N * N^2)|\n|<= [18..22]| *O*(2^N * N)|\n|<= [100]| *O*(N^4)|\n|<= [400]| *O*(N^3)|\n|<= [2*K*]| *O*(N^2 * log(N))|\n|<= [10*K*]| *O*(N^2)|\n|<= [1*M*]| *O*(N * log(N))|\n|<= [100*M*]| *O*(N), *O*(logN), *O*(1)|\n\n",
    "left": "",
    "right": "Time and Space",
    "name": "time_and_space.md",
    "state": false
  },
  "linear_search.md": {
    "content": "# Linear Search\n\n**Linear search** is a sequential searching algorithm where we start from one end and check every element of the list until the desired element is found. It is the simplest searching algorithm.\n\n---\n\n### How Linear Search Works?\n\nThe following steps are followed to search for an element `k = 1` in the list below.\n\n![notation](linear_search_arr0.webp)\n\n1. Start from the first element, compare `k` with each element `x`.\n\n    ![notation](linear_search_arr1.webp)\n2. If `x == k`, return the index.\n\n    ![notation](linear_search_arr2.webp)\n3. Else, return `not found`.\n\n---\n\n### Linear Search Algorithm\n\n    LinearSearch(array, key)\n        for each item in the array\n            if item == value\n                return its index\n\n### Implementation\n\n    int search(vector<int>& array, int n, int x) {\n        for (int i = 0; i < n; i++)\n            if (array[i] == x)\n                return i;\n        return -1;\n    }\n---\n\n### Linear Search Complexities\n\n**Time Complexity:** `O(N)`\\\n**Space Complexity:** `O(1)`\n\n---\n### Linear Search Applications\nFor searching operations in smaller arrays (<100 items).",
    "left": "Search Algorithms",
    "right": "Linear search",
    "name": "linear_search.md",
    "state": false
  },
  "binary_search.md": {
    "content": "# Binary Search\n\n**Binary Search** is an efficient searching algorithm for finding an element's position in a sorted array.\n\nIf all the names in the world are written down together in order and you want to search for the position of a specific name, binary search will accomplish this in a maximum of 35 iterations.\n\nWhen binary search is used to perform operations on a sorted set, the number of iterations can always be reduced on the basis of the value that is being searched.\n\nLet us consider the following array:\n\n![notation](array.png)\n\nBy using linear search, the position of element 8 will be determined in the 9th iteration.\n\nLet's see how the number of iterations can be reduced by using binary search. Before we start the search, we need to know the start and end of the range. Lets call them Low and High.\n\n    Low = 0\n    High = n - 1\n\nNow, compare the search value *K* with the element located at the median of the lower and upper bounds. If the value *K* is greater, increase the lower bound, else decrease the upper bound.\n\n![notation](array1.png)\n\nReferring to the image above, the lower bound is 0 and the upper bound is 9. The median of the lower and upper bounds is (lower_bound + upper_bound) / 2 = 4. Here a[4] = 4. The value 4>2, which is the value that you are searching for. Therefore, we do not need to conduct a search on any element beyond 4 as the elements beyond it will obviously be greater than 2.\n\nTherefore, we can always drop the upper bound of the array to the position of element 4. Now, we follow the same procedure on the same array with the following values:\n\n    Low: 0\n    High: 3\n\nRepeat this procedure recursively until Low > High. If at any iteration, we get *a[mid] = key*, we return value of *mid*. This is the position of *key* in the array. If *key* is not present in the array, we return *-1*.\n\n### Implementation (Recursive)\n\n    int binarySearch(vector<int>& array, int x, int low, int high) {\n        if (high >= low) {\n            int mid = low + (high - low) / 2;\n\n            if (array[mid] == x)\n                return mid;\n\n            if (array[mid] > x)\n                return binarySearch(array, x, low, mid - 1);\n\n            return binarySearch(array, x, mid + 1, high);\n        }\n        return -1;\n    }\n\n### Implementation (Iterative)\n\n    int binarySearch(vector<int>& array, int x, int low, int high) {\n        while (low <= high) {\n            int mid = low + (high - low) / 2;\n\n            if (array[mid] == x)\n                return mid;\n\n            if (array[mid] < x)\n                low = mid + 1;\n\n            else\n                high = mid - 1;\n        }\n        return -1;\n    }\n\n### Time complexity\n\nAs we dispose off one part of the search case during every step of binary search, and perform the search operation on the other half, this results in a worst case time complexity of *O*(logN).",
    "left": "",
    "right": "Binary search",
    "name": "binary_search.md",
    "state": false
  },
  "bubble_sort.md": {
    "content": "# Bubble Sort\n\n**Bubble sort** is a sorting algorithm that compares two adjacent elements and swaps them until they are not in the intended order.\n\nJust like the movement of air bubbles in the water that rise up to the surface, each element of the array move to the end in each iteration. Therefore, it is called a bubble sort.\n\n### Working of Bubble Sort\n\nSuppose we are trying to sort the elements in **ascending order**.\n\n**1. First Iteration (Compare and Swap)**\n\n1. Starting from the first index, compare the first and the second elements.\n2. If the first element is greater than the second element, they are swapped.\n3. Now, compare the second and the third elements. Swap them if they are not in order.\n4. The above process goes on until the last element.\n\n![notation](bubble_sort.webp)\n\n**2. Remaining Iteration**\n\nThe same process goes on for the remaining iterations.\n\nAfter each iteration, the largest element among the unsorted elements is placed at the end.\n\n![notation](bubble_sort2.webp)\n\nIn each iteration, the comparison takes place up to the last unsorted element.\n\n![notation](bubble_sort3.webp)\n\nThe array is sorted when all the unsorted elements are placed at their correct positions.\n\n![notation](bubble_sort4.webp)\n\n### Implementation\n\n    void bubbleSort(vector<int>& array) {\n        for (int i = 0; i < array.size(); i++)\n            for (int j = 0; j < array.size() - i; j++)\n                if (array[j] > array[j + 1])\n                    swap(array[j], array[j + 1]);\n    }\n\n### Time complexity\n\nThe number of comparisons that we will perform using bubble sort algorithm is (n-1) + (n-2) + (n-3) +.....+ 1 = n(n-1)/2, hence the time complexity will be *O*(N^2).",
    "left": "Sorting algorithms",
    "right": "Bubble sort",
    "name": "bubble_sort.md",
    "state": false
  },
  "insertion_sort.md": {
    "content": "# Insertion Sort\n\n**Insertion sort** is a sorting algorithm that places an unsorted element at its suitable place in each iteration.\n\nInsertion sort works similarly as we sort cards in our hand in a card game.\n\nWe assume that the first card is already sorted then, we select an unsorted card. If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. In the same way, other unsorted cards are taken and put in their right place.\n\nA similar approach is used by insertion sort.\n\n### Working of Insertion Sort\n\nSuppose we need to sort the following array.\n\n![notation](ins_sort_arr.webp)\n\n1. The first element in the array is assumed to be sorted. Take the second element and store it separately in key.\n\nCompare key with the first element. If the first element is greater than key, then key is placed in front of the first element.\n\n![notation](ins_sort_arr1.webp)\n\n2. Now, the first two elements are sorted.\n\nTake the third element and compare it with the elements on the left of it. Placed it just behind the element smaller than it. If there is no element smaller than it, then place it at the beginning of the array.\n\n![notation](ins_sort_arr2.webp)\n\n3. Similarly, place every unsorted element at its correct position.\n\n![notation](ins_sort_arr3.webp)\\\n![notation](ins_sort_arr4.webp)\n\n### Implementation\n\n    void insertionSort(vector<int>& array) {\n        for (int i = 1; i < array.size(); i++) {\n            int key = array[i];\n            int j = i - 1;\n                while (key < array[j] && j >= 0) {\n                array[j + 1] = array[j];\n                --j;\n                }\n            array[j + 1] = key;\n        }\n    }\n\n### Time complexity\n\n**Worst Case Complexity: *O*(n^2)**\nSuppose, an array is in ascending order, and you want to sort it in descending order. In this case, worst case complexity occurs.\n\nEach element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.\n\nThus, the total number of comparisons = n*(n-1) ~ n2\n\n**Best Case Complexity: *O*(n)**\nWhen the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. So, there are only n number of comparisons. Thus, complexity is linear.\n\n**Average Case Complexity: *O*(n^2)**\nIt occurs when the elements of an array are in jumbled order (neither ascending nor descending).\n\n### Space Complexity\n\nSpace complexity is *O*(1) because an extra variable key is used.\n\n### Insertion Sort Applications\nThe insertion sort is used when:\n\n* The array is has a small number of elements\n* There are only a few elements left to be sorted",
    "left": "",
    "right": "Insertion sort",
    "name": "insertion_sort.md",
    "state": false
  },
  "merge_sort.md": {
    "content": "# Merge Sort\n\n**Merge Sort** is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm.\n\nHere, a problem is divided into multiple sub-problems. Each sub-problem is solved individually. Finally, sub-problems are combined to form the final solution.\n\n![notation](merge_sort_arr1.webp)\n\n### **Divide and Conquer Strategy**\n\nUsing the **Divide and Conquer** technique, we divide a problem into subproblems. When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem.\n\nSuppose we had to sort an array `A`. A subproblem would be to sort a sub-section of this array starting at index `p` and ending at index `r`, denoted as `A[p..r]`.\n\n**Divide**\n\nIf q is the half-way point between p and r, then we can split the subarray `A[p..r]` into two arrays `A[p..q]` and `A[q+1, r]`.\n\n**Conquer**\n\nIn the conquer step, we try to sort both the subarrays `A[p..q]` and `A[q+1, r]`. If we haven't yet reached the base case, we again divide both these subarrays and try to sort them.\n\n**Combine**\n\nWhen the conquer step reaches the base step and we get two sorted subarrays `A[p..q]` and `A[q+1, r]` for array `A[p..r]`, we combine the results by creating a sorted array `A[p..r]` from two sorted subarrays `A[p..q]` and `A[q+1, r]`.\n\n___\n\n### Merge Sort Algorithm\n\nThe MergeSort function repeatedly divides the array into two halves until we reach a stage where we try to perform MergeSort on a subarray of size 1 i.e. `p == r`.\n\nAfter that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged.\n\n    MergeSort(A, p, r):\n        if p > r \n            return\n        q = (p+r)/2\n        mergeSort(A, p, q)\n        mergeSort(A, q+1, r)\n        merge(A, p, q, r)\n\nTo sort an entire array, we need to call `MergeSort(A, 0, length(A)-1)`.\n\nAs shown in the image below, the merge sort algorithm recursively divides the array into halves until we reach the base case of array with 1 element. After that, the merge function picks up the sorted sub-arrays and merges them to gradually sort the entire array.\n\n![notation](merge_sort_arr2.webp)\n\n**The merge Step of Merge Sort**\n\nEvery recursive algorithm is dependent on a base case and the ability to combine the results from base cases. Merge sort is no different. The most important part of the merge sort algorithm is, you guessed it, `merge` step.\n\nThe merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array).\n\nThe algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array.\n\n    Have we reached the end of any of the arrays?\n        No:\n            Compare current elements of both arrays \n            Copy smaller element into sorted array\n            Move pointer of element containing smaller element\n        Yes:\n            Copy all remaining elements of non-empty array\n\n![notation](merge_sort_arr3.webp)\n\n### Writing the Code for Merge Algorithm\n\nA noticeable difference between the merging step we described above and the one we use for merge sort is that we only perform the merge function on consecutive sub-arrays.\n\nThis is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) and the last index of the second subarray.\n\nOur task is to merge two subarrays `A[p..q]` and `A[q+1..r]` to create a sorted array `A[p..r]`. So the inputs to the function are A, p, q and r\n\nThe merge function works as follows:\n\n1. Create copies of the subarrays `L ← A[p..q]` and `M ← A[q+1..r]`.\n2. Create three pointers `i`, `j` and `k`\n* `i` maintains current index of `L`, starting at 1\n* `j` maintains current index of `M`, starting at 1\n* `k` maintains the current index of `A[p..q]`, starting at `p`.\n3. Until we reach the end of either `L` or `M`, pick the larger among the elements from `L` and `M` and place them in the correct position at `A[p..q]`\n4. When we run out of elements in either `L` or `M`, pick up the remaining elements and put in `A[p..q]`\n\nIn code, this would look like:\n\n    // Merge two subarrays L and M into arr\n    void merge(vector<int>& arr, int p, int q, int r) {\n\n        // Create L ← A[p..q] and M ← A[q+1..r]\n        int n1 = q - p + 1;\n        int n2 = r - q;\n\n        vector<int> L(n1), M(n2);\n\n        for (int i = 0; i < n1; i++)\n            L[i] = arr[p + i];\n        for (int j = 0; j < n2; j++)\n            M[j] = arr[q + 1 + j];\n\n        // Maintain current index of sub-arrays and main array\n        int i, j, k;\n        i = 0;\n        j = 0;\n        k = p;\n\n        // Until we reach either end of either L or M, pick larger among\n        // elements L and M and place them in the correct position at A[p..r]\n        while (i < n1 && j < n2) {\n            if (L[i] <= M[j]) {\n                arr[k] = L[i];\n                i++;\n            } else {\n                arr[k] = M[j];\n                j++;\n            }\n            k++;\n        }\n\n        // When we run out of elements in either L or M,\n        // pick up the remaining elements and put in A[p..r]\n        while (i < n1) {\n            arr[k] = L[i];\n            i++;\n            k++;\n        }\n\n        while (j < n2) {\n            arr[k] = M[j];\n            j++;\n            k++;\n        }\n    }\n\n### Merge( ) Function Explained Step-By-Step\n\nA lot is happening in this function, so let's take an example to see how this would work.\n\nAs usual, a picture speaks a thousand words.\n\n![notation](merge_sort_arr4.webp)\n\nThe array `A[0..5]` contains two sorted subarrays `A[0..3]` and `A[4..5]`. Let us see how the merge function will merge the two arrays.\n\n    void merge(vector<int>& arr, int p, int q, int r) {\n    // Here, p = 0, q = 4, r = 6 (size of array)\n\n**Step 1: Create duplicate copies of sub-arrays to be sorted**\n\n    // Create L ← A[p..q] and M ← A[q+1..r]\n    int n1 = q - p + 1 = 3 - 0 + 1 = 4;\n    int n2 = r - q = 5 - 3 = 2;\n\n    vector<int> L(4), M(2);\n\n    for (int i = 0; i < 4; i++)\n        L[i] = arr[p + i];\n        // L[0,1,2,3] = A[0,1,2,3] = [1,5,10,12]\n\n    for (int j = 0; j < 2; j++)\n        M[j] = arr[q + 1 + j];\n        // M[0,1] = A[4,5] = [6,9]\n\n![notation](merge_sort_arr5.webp)\n\n**Step 2: Maintain current index of sub-arrays and main array**\n\n    int i, j, k;\n    i = 0; \n    j = 0; \n    k = p; \n\n![notation](merge_sort_arr6.webp)\n\n**Step 3: Until we reach the end of either L or M, pick larger among elements L and M and place them in the correct position at A[p..r]**\n\n    while (i < n1 && j < n2) { \n        if (L[i] <= M[j]) { \n            arr[k] = L[i]; \n            i++; \n        } \n        else { \n            arr[k] = M[j]; \n            j++; \n        } \n        k++; \n    }\n![notation](merge_sort_arr7.webp)\n\n**Step 4: When we run out of elements in either L or M, pick up the remaining elements and put in A[p..r]**\n\n    // We exited the earlier loop because j < n2 doesn't hold\n    while (i < n1)\n    {\n        arr[k] = L[i];\n        i++;\n        k++;\n    }\n\n![notation](merge_sort_arr8.webp)\n\n        // We exited the earlier loop because i < n1 doesn't hold  \n        while (j < n2)\n        {\n            arr[k] = M[j];\n            j++;\n            k++;\n        }\n    }\n\n![notation](merge_sort_arr9.webp)\n\nThis step would have been needed if the size of M was greater than L.\n\nAt the end of the merge function, the subarray `A[p..r]` is sorted.\n\n### Full Implementation\n\n    // Merge two subarrays L and M into arr\n    void merge(vector<int>& arr, int p, int q, int r) {\n    \n    // Create L ← A[p..q] and M ← A[q+1..r]\n    int n1 = q - p + 1;\n    int n2 = r - q;\n\n    vector<int> L(n1), M(n2);\n\n    for (int i = 0; i < n1; i++)\n        L[i] = arr[p + i];\n    for (int j = 0; j < n2; j++)\n        M[j] = arr[q + 1 + j];\n\n    // Maintain current index of sub-arrays and main array\n    int i, j, k;\n    i = 0;\n    j = 0;\n    k = p;\n\n    // Until we reach either end of either L or M, pick larger among\n    // elements L and M and place them in the correct position at A[p..r]\n    while (i < n1 && j < n2) {\n        if (L[i] <= M[j]) {\n            arr[k] = L[i];\n            i++;\n        } else {\n            arr[k] = M[j];\n            j++;\n        }\n        k++;\n    }\n\n    // When we run out of elements in either L or M,\n    // pick up the remaining elements and put in A[p..r]\n    while (i < n1) {\n        arr[k] = L[i];\n        i++;\n        k++;\n    }\n\n        while (j < n2) {\n            arr[k] = M[j];\n            j++;\n            k++;\n        }\n    }\n\n        // Divide the array into two subarrays, sort them and merge them\n    void mergeSort(vector<int>& arr, int l, int r) {\n        if (l < r) {\n            // m is the point where the array is divided into two subarrays\n            int m = l + (r - l) / 2;\n\n            mergeSort(arr, l, m);\n            mergeSort(arr, m + 1, r);\n\n            // Merge the sorted subarrays\n            merge(arr, l, m, r);\n        }\n    }\n\n**Merge Sort Complexity**\n\n|**Time Complexity** | |\n|---|---|\n|Best| *O*(N * log(N))|\n|Worst| *O*(N * log(N))|\n|Average| *O*(N * log(N))|\n|**Space Complexity**| *O*(N)|\n|**Stability**| Yes|\n\n### Merge Sort Applications\n\n* Inversion count problem\n* External sorting\n* E-commerce applications",
    "left": "",
    "right": "Merge sort",
    "name": "merge_sort.md",
    "state": false
  },
  "heap_sort.md": {
    "content": "# Heap Sort\n\n**Heap Sort** is a popular and efficient sorting algorithm in computer programming. Learning how to write the heap sort algorithm requires knowledge of two types of data structures - arrays and trees.\n\nThe initial set of numbers that we want to sort is stored in an array e.g. `[10,3,76,34,23,32]` and after sorting, we get a sorted array `[3,10,23,32,34,76]`.\n\nHeap sort works by visualizing the elements of the array as a special kind of complete binary tree called a heap.\n\n> Note: As a prerequisite, you must know about a complete [binary tree][1] and [heap data structure][2].\n\n### Relationship between Array Indexes and Tree Elements\n\nA complete binary tree has an interesting property that we can use to find the children and parents of any node.\n\nIf the index of any element in the array is `i`, the element in the index `2i+1` will become the left child and element in `2i+2` index will become the right child. Also, the parent of any element at index `i` is given by the lower bound of `(i-1)/2`.\n\n![notation](heap_sort_arr0.webp)\n\nLet's test it out,\n\n    Left child of 1 (index 0)\n    = element in (2*0+1) index \n    = element in 1 index \n    = 12\n\n\n    Right child of 1\n    = element in (2*0+2) index\n    = element in 2 index \n    = 9\n\n    Similarly,\n    Left child of 12 (index 1)\n    = element in (2*1+1) index\n    = element in 3 index\n    = 5\n\n    Right child of 12\n    = element in (2*1+2) index\n    = element in 4 index\n    = 6\n\nLet us also confirm that the rules hold for finding parent of any node\n\n    Parent of 9 (position 2) \n    = (2-1)/2 \n    = ½ \n    = 0.5\n    ~ 0 index \n    = 1\n\n    Parent of 12 (position 1) \n    = (1-1)/2 \n    = 0 index \n    = 1\n\nUnderstanding this mapping of array indexes to tree positions is critical to understanding how the Heap Data Structure works and how it is used to implement Heap Sort.\n\n---\n\n### What is Heap Data Structure?\n\nHeap is a special tree-based data structure. A binary tree is said to follow a heap data structure if\n\n* it is a complete [binary tree][1]\n* All nodes in the tree follow the property that they are greater than their children i.e. the largest element is at the root and both its children and smaller than the root and so on. Such a heap is called a max-heap. If instead, all nodes are smaller than their children, it is called a min-heap\n\nThe following example diagram shows Max-Heap and Min-Heap.\n\n![notation](heap_sort_arr1.webp)\n\nTo learn more about it, please visit [Heap Data Structure][2].\n\n---\n### How to \"heapify\" a tree\n\nStarting from a complete binary tree, we can modify it to become a Max-Heap by running a function called heapify on all the non-leaf elements of the heap.\n\nSince heapify uses recursion, it can be difficult to grasp. So let's first think about how you would heapify a tree with just three elements.\n\n    heapify(array)\n        Root = array[0]\n        Largest = largest( array[0] , array [2*0 + 1]. array[2*0+2])\n        if(Root != Largest)\n            Swap(Root, Largest)\n\n![notation](heap_sort_arr2.webp)\n\nThe example above shows two scenarios - one in which the root is the largest element and we don't need to do anything. And another in which the root had a larger element as a child and we needed to swap to maintain max-heap property.\n\nIf you're worked with recursive algorithms before, you've probably identified that this must be the base case.\n\nNow let's think of another scenario in which there is more than one level.\n\n![notation](heap_sort_arr3.webp)\n\nThe top element isn't a max-heap but all the sub-trees are max-heaps.\n\nTo maintain the max-heap property for the entire tree, we will have to keep pushing 2 downwards until it reaches its correct position.\n\n![notation](heap_sort_arr4.webp)\n\nThus, to maintain the max-heap property in a tree where both sub-trees are max-heaps, we need to run heapify on the root element repeatedly until it is larger than its children or it becomes a leaf node.\n\nWe can combine both these conditions in one heapify function as\n\n    void heapify(vector<int>& arr, int n, int i) {\n        // Find largest among root, left child and right child\n        int largest = i;\n        int left = 2 * i + 1;\n        int right = 2 * i + 2;\n\n        if (left < n && arr[left] > arr[largest])\n            largest = left;\n\n        if (right < n && arr[right] > arr[largest])\n            largest = right;\n\n            // Swap and continue heapifying if root is not largest\n            if (largest != i) {\n            swap(arr[i], arr[largest]);\n            heapify(arr, n, largest);\n        }\n    }\n\nThis function works for both the base case and for a tree of any size. We can thus move the root element to the correct position to maintain the max-heap status for any tree size as long as the sub-trees are max-heaps.\n\n---\n\n### Build max-heap\n\nTo build a max-heap from any tree, we can thus start heapifying each sub-tree from the bottom up and end up with a max-heap after the function is applied to all the elements including the root element.\n\nIn the case of a complete tree, the first index of a non-leaf node is given by n/2 - 1. All other nodes after that are leaf-nodes and thus don't need to be heapified.\n\nSo, we can build a maximum heap as\n\n    // Build heap (rearrange array)\n    for (int i = n / 2 - 1; i >= 0; i--)\n      heapify(arr, n, i);\n    \n![notation](heap_sort_arr5.webp)\n![notation](heap_sort_arr6.webp)\n![notation](heap_sort_arr7.webp)\n![notation](heap_sort_arr8.webp)\n\nAs shown in the above diagram, we start by heapifying the lowest smallest trees and gradually move up until we reach the root element.\n\nIf you've understood everything till here, congratulations, you are on your way to mastering the Heap sort.\n\n### Working of Heap Sort\n\n1. Since the tree satisfies Max-Heap property, then the largest item is stored at the root node.\n\n2. **Swap:** Remove the root element and put at the end of the array (nth position) Put the last item of the tree (heap) at the vacant place.\n\n3. **Remove:** Reduce the size of the heap by 1.\n\n4. **Heapify:** Heapify the root element again so that we have the highest element at root.\n\n5. The process is repeated until all the items of the list are sorted.\n\n![notation](heap_sort_arr9.webp)\n\nThe code below shows the operation.\n\n    // Heap sort\n    for (int i = n - 1; i >= 0; i--) {\n      swap(arr[0], arr[i]);\n\n      // Heapify root element to get highest element at root again\n      heapify(arr, i, 0);\n    }\n\n### Implementation\n  \n    void heapify(vector<int>& arr, int n, int i) {\n        // Find largest among root, left child and right child\n        int largest = i;\n        int left = 2 * i + 1;\n        int right = 2 * i + 2;\n\n        if (left < n && arr[left] > arr[largest])\n            largest = left;\n\n        if (right < n && arr[right] > arr[largest])\n            largest = right;\n\n        // Swap and continue heapifying if root is not largest\n        if (largest != i) {\n            swap(arr[i], arr[largest]);\n            heapify(arr, n, largest);\n        }\n    }\n  \n    // main function to do heap sort\n    void heapSort(vector<int> arr, int n) {\n        // Build max heap\n        for (int i = n / 2 - 1; i >= 0; i--)\n            heapify(arr, n, i);\n\n        // Heap sort\n        for (int i = n - 1; i >= 0; i--) {\n            swap(arr[0], arr[i]);\n\n            // Heapify root element to get highest element at root again\n            heapify(arr, i, 0);\n        }\n    }\n\n**Heap Sort Complexity**\n\n|**Time Complexity** | |\n|---|---|\n|Best| *O*(N * log(N))|\n|Worst| *O*(N * log(N))|\n|Average| *O*(N * log(N))|\n|**Space Complexity**| *O*(1)|\n|**Stability**| No|\n\nHeap Sort has `O(N * log(N))` time complexities for all the cases ( best case, average case, and worst case).\n\nLet us understand the reason why. The height of a complete binary tree containing n elements is `log(N)`.\n\nAs we have seen earlier, to fully heapify an element whose subtrees are already max-heaps, we need to keep comparing the element with its left and right children and pushing it downwards until it reaches a point where both its children are smaller than it.\n\nIn the worst case scenario, we will need to move an element from the root to the leaf node making a multiple of `log(N)` comparisons and swaps.\n\nDuring the build_max_heap stage, we do that for `N / 2` elements so the worst case complexity of the build_heap step is `N / 2*log(N) ~ N * log(N)`.\n\nDuring the sorting step, we exchange the root element with the last element and heapify the root element. For each element, this again takes `log(N)` worst time because we might have to bring the element all the way from the root to the leaf. Since we repeat this `N` times, the heap_sort step is also `N * log(N)`.\n\nAlso since the `build_max_heap` and `heap_sort` steps are executed one after another, the algorithmic complexity is not multiplied and it remains in the order of `N * log(N)`.\n\nAlso it performs sorting in `O(1)` space complexity. Compared with Quick Sort, it has a better worst case `O(N * log(N))`. Quick Sort has complexity `O(N^2)` for worst case. But in other cases, Quick Sort is fast. Introsort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.\n\n---\n\n### Heap Sort Applications\n\nSystems concerned with security and embedded systems such as Linux Kernel use Heap Sort because of the `O(N * log(N))` upper bound on Heapsort's running time and constant `O(1)` upper bound on its auxiliary storage.\n\nAlthough Heap Sort has `O(N * log(N))` time complexity even for the worst case, it doesn't have more applications ( compared to other sorting algorithms like Quick Sort, Merge Sort ). However, its underlying data structure, heap, can be efficiently used if we want to extract the smallest (or largest) from the list of items without the overhead of keeping the remaining items in the sorted order. For e.g Priority Queues.\n\n[1]: <https://empty_page1 \"binary tree\">\n[2]: <https://empty_page2 \"heap data structure\">\n",
    "left": "",
    "right": "Heap sort",
    "name": "heap_sort.md",
    "state": false
  },
  "quick_sort.md": {
    "content": "# Quicksort\n\n**Quicksort** is a sorting algorithm based on the **divide and conquer approach** where\n\n1. An array is divided into subarrays by selecting a **pivot element** (element selected from the array).\n\n    While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot.\n\n2. The left and right subarrays are also divided using the same approach. This process continues until each subarray contains a single element.\n\n3. At this point, elements are already sorted. Finally, elements are combined to form a sorted array.\n\n### Working of Quicksort\n\n**1. Select the Pivot Element**\n\nThere are different variations of quicksort where the pivot element is selected from different positions. Here, we will be selecting the rightmost element of the array as the pivot element.\n\n![notation](quick_sort_arr0.webp)\n\n**2. Rearrange the Array**\n\nNow the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left and the elements greater than the pivot are put on the right.\n\n![notation](quick_sort_arr1.webp)\n\nHere's how we rearrange the array:\n\n1. A pointer is fixed at the pivot element. The pivot element is compared with the elements beginning from the first index.\n\n![notation](quick_sort_arr2.webp)\n\n2. If the element is greater than the pivot element, a second pointer is set for that element.\n\n![notation](quick_sort_arr3.webp)\n\n3. Now, pivot is compared with other elements. If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.\n\n![notation](quick_sort_arr4.webp)\n\n4. Again, the process is repeated to set the next greater element as the second pointer. And, swap it with another smaller element.\n\n![notation](quick_sort_arr5.webp)\n\n5. The process goes on until the second last element is reached.\n\n![notation](quick_sort_arr6.webp)\n\n6. Finally, the pivot element is swapped with the second pointer.\n\n![notation](quick_sort_arr7.webp)\n\n**3. Divide Subarrays**\n\nPivot elements are again chosen for the left and the right sub-parts separately. And, **step 2** is repeated.\n\n![notation](quick_sort_arr8.webp)\n\nThe subarrays are divided until each subarray is formed of a single element. At this point, the array is already sorted.\n\n### Quick Sort Algorithm\n\n    quickSort(array, leftmostIndex, rightmostIndex)\n    if (leftmostIndex < rightmostIndex)\n        pivotIndex <- partition(array,leftmostIndex, rightmostIndex)\n        quickSort(array, leftmostIndex, pivotIndex - 1)\n        quickSort(array, pivotIndex, rightmostIndex)\n\n    partition(array, leftmostIndex, rightmostIndex)\n    set rightmostIndex as pivotIndex\n    storeIndex <- leftmostIndex - 1\n    for i <- leftmostIndex + 1 to rightmostIndex\n    if element[i] < pivotElement\n        swap element[i] and element[storeIndex]\n        storeIndex++\n    swap pivotElement and element[storeIndex+1]\n    return storeIndex + 1\n\n### Visual Illustration of Quicksort Algorithm\n\nYou can understand the working of quicksort algorithm with the help of the illustrations below.\n\n![notation](quick_sort_arr9.webp)\n![notation](quick_sort_arr10.webp)\n\n### Implementation\n\n    // function to rearrange array (find the partition point)\n    int partition(vector<int>& array, int low, int high) {  \n        // select the rightmost element as pivot\n        int pivot = array[high];\n        \n        // pointer for greater element\n        int i = low - 1;\n\n        // traverse each element of the array\n        // compare them with the pivot\n        for (int j = low; j < high; j++) {\n            if (array[j] <= pivot) {    \n                // if element smaller than pivot is found\n                // swap it with the greater element pointed by i\n                i++;\n                \n                // swap element at i with element at j\n                swap(array[i], array[j]);\n            }\n        }\n        \n        // swap pivot with the greater element at i\n        swap(array[i + 1], array[high]);\n        \n        // return the partition point\n        return i + 1;\n    }\n\n    void quickSort(vector<int> array, int low, int high) {\n        if (low < high) {\n            // find the pivot element such that\n            // elements smaller than pivot are on left of pivot\n            // elements greater than pivot are on righ of pivot\n            int pi = partition(array, low, high);\n\n            // recursive call on the left of pivot\n            quickSort(array, low, pi - 1);\n\n            // recursive call on the right of pivot\n            quickSort(array, pi + 1, high);\n        }\n    }\n\n**Quick Sort Complexity**\n\n|**Time Complexity** | |\n|---|---|\n|Best| *O*(N * log(N))|\n|Worst| *O*(N^2|\n|Average| *O*(N * log(N))|\n|**Space Complexity**| *O*(log(N))|\n|**Stability**| No|\n\n**1. Time Complexities**\n\n* **Worst Case Complexity [Big-O]:** `O(N^2)`\n\n    It occurs when the pivot element picked is either the greatest or the smallest element.\n\n    This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. One sub-array is always empty and another sub-array contains `N - 1` elements. Thus, quicksort is called only on this sub-array.\n\n    However, the quicksort algorithm has better performance for scattered pivots.\n\n* **Best Case Complexity [Big-omega]:** `O(N*log(N))`\n\n    It occurs when the pivot element is always the middle element or near to the middle element.\n\n* **Average Case Complexity [Big-theta]:** `O(N*log(N))`\n\n    It occurs when the above conditions do not occur.\n\n**2. Space Complexity**\n\nThe space complexity for quicksort is `O(log(N))`.\n\n---\n**Quicksort Applications**\n\nQuicksort algorithm is used when\n\n* The programming language is good for recursion\n* Time complexity matters\n* Space complexity matters",
    "left": "",
    "right": "Quick sort",
    "name": "quick_sort.md",
    "state": false
  },
  "dfs.md": {
    "content": "# Depth First Search (DFS)\n\n**Depth first Search or Depth first traversal** is a recursive algorithm for searching all the vertices of a graph or tree data structure. Traversal means visiting all the nodes of a graph.\n\n---\n\n### Depth First Search Algorithm\n\nA standard DFS implementation puts each vertex of the graph into one of two categories:\n\n1. Visited\n2. Not Visited\n\nThe purpose of the algorithm is to mark each vertex as visited while avoiding cycles.\n\nThe DFS algorithm works as follows:\n\n1. Start by putting any one of the graph's vertices on top of a stack.\n2. Take the top item of the stack and add it to the visited list.\n3. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the top of the stack.\n4. Keep repeating steps 2 and 3 until the stack is empty.\n\n---\n\n### Depth First Search Example\n\nLet's see how the Depth First Search algorithm works with an example. We use an undirected graph with 5 vertices.\n\n![notation](dfs0.webp)\n\nWe start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack.\n\n![notation](dfs1.webp)\n\nNext, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.\n\n![notation](dfs2.webp)\n\nVertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it.\n\n![notation](dfs3.webp)\n\n![notation](dfs4.webp)\n\nAfter we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph.\n\n![notation](dfs5.webp)\n\n---\n\n### DFS Pseudocode (recursive implementation)\n\nThe pseudocode for DFS is shown below. In the init() function, notice that we run the DFS function on every node. This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the DFS algorithm on every node.\n\n    DFS(G, u)\n        u.visited = true\n        For each v ∈ G.Adj[u]\n            If v.visited == false\n                DFS(G,v)\n        \n    init() {\n        For each u ∈ G\n            u.visited = false\n        For each u ∈ G\n        DFS(G, u)\n    }\n\n---\n\n### Implementation\n\n    class Graph {\n        int numVertices;\n        list<int> *adjLists;\n        bool *visited;\n\n    public:\n        Graph(int V);\n        void addEdge(int src, int dest);\n        void DFS(int vertex);\n    };\n\n    // Initialize graph\n    Graph::Graph(int vertices) {\n        numVertices = vertices;\n        adjLists = new list<int>[vertices];\n        visited = new bool[vertices];\n    }\n\n    // Add edges\n    void Graph::addEdge(int src, int dest) {\n        adjLists[src].push_front(dest);\n    }\n\n    // DFS algorithm\n    void Graph::DFS(int vertex) {\n        visited[vertex] = true;\n        list<int> adjList = adjLists[vertex];\n\n        cout << vertex << \" \";\n\n        list<int>::iterator i;\n        for (i = adjList.begin(); i != adjList.end(); ++i)\n            if (!visited[*i])\n                DFS(*i);\n    }\n\n---\n\n### Complexity of Depth First Search\n\nThe time complexity of the DFS algorithm is represented in the form of `O(V + E)`, where `V` is the number of nodes and `E` is the number of edges.\n\nThe space complexity of the algorithm is `O(V)`.\n\n---\n\n### Application of DFS Algorithm\n\n1. For finding the path\n2. To test if the graph is bipartite\n3. For finding the strongly connected components of a graph\n4. For detecting cycles in a graph",
    "left": "Graph Theory",
    "right": "Dfs",
    "name": "dfs.md",
    "state": false
  },
  "bfs.md": {
    "content": "# Breadth First Search (BFS)\n\n**Breadth First Traversal or Breadth First Search** is an iterative algorithm for searching all the vertices of a graph or tree data structure.\n\n---\n\n### BFS algorithm\n\nA standard BFS implementation puts each vertex of the graph into one of two categories:\n\n1. Visited\n2. Not Visited\n\nThe purpose of the algorithm is to mark each vertex as visited while avoiding cycles.\n\nThe algorithm works as follows:\n\n1. Start by putting any one of the graph's vertices at the back of a queue.\n2. Take the front item of the queue and add it to the visited list.\n3. Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue.\n4. Keep repeating steps 2 and 3 until the queue is empty.\n\nThe graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node\n\n---\n\n### BFS example\n\nLet's see how the Breadth First Search algorithm works with an example. We use an undirected graph with 5 vertices.\n\n![notation](bfs0.webp)\n\nWe start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the queue.\n\n![notation](bfs1.webp)\n\nNext, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.\n\n![notation](bfs2.webp)\n\nVertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue.\n\n![notation](bfs3.webp)\n\n![notation](bfs4.webp)\n\nOnly 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it.\n\n![notation](bfs5.webp)\n\nSince the queue is empty, we have completed the Breadth First Traversal of the graph.\n\n---\n\n### BFS pseudocode\n\n    create a queue Q \n    mark v as visited and put v into Q \n    while Q is non-empty \n        remove the head u of Q \n        mark and enqueue all (unvisited) neighbours of u\n\n---\n\n### Implementation\n\n    class Graph {\n        int numVertices;\n        list<int>* adjLists;\n        bool* visited;\n\n    public:\n        Graph(int vertices);\n        void addEdge(int src, int dest);\n        void BFS(int startVertex);\n    };\n\n    // Create a graph with given vertices,\n    // and maintain an adjacency list\n    Graph::Graph(int vertices) {\n        numVertices = vertices;\n        adjLists = new list<int>[vertices];\n    }\n\n    // Add edges to the graph\n    void Graph::addEdge(int src, int dest) {\n        adjLists[src].push_back(dest);\n        adjLists[dest].push_back(src);\n    }\n\n    // BFS algorithm\n    void Graph::BFS(int startVertex) {\n        visited = new bool[numVertices];\n        for (int i = 0; i < numVertices; i++)\n            visited[i] = false;\n\n        list<int> queue;\n\n        visited[startVertex] = true;\n        queue.push_back(startVertex);\n\n        list<int>::iterator i;\n\n        while (!queue.empty()) {\n            int currVertex = queue.front();\n            cout << \"Visited \" << currVertex << \" \";\n            queue.pop_front();\n\n            for (i = adjLists[currVertex].begin(); i != adjLists[currVertex].end(); ++i) {\n                int adjVertex = *i;\n                if (!visited[adjVertex]) {\n                    visited[adjVertex] = true;\n                    queue.push_back(adjVertex);\n                }\n            }\n        }\n    }\n\n---\n\n### BFS Algorithm Complexity\n\nThe time complexity of the BFS algorithm is represented in the form of `O(V + E)`, where `V` is the number of nodes and `E` is the number of edges.\n\nThe space complexity of the algorithm is `O(V)`.\n\n---\n\n### BFS Algorithm Applications\n\n1. To build index by search index\n2. For GPS navigation\n3. Path finding algorithms\n4. In Ford-Fulkerson algorithm to find maximum flow in a network\n5. Cycle detection in an undirected graph\n6. In minimum spanning tree\n",
    "left": "",
    "right": "Bfs",
    "name": "bfs.md",
    "state": false
  },
  "dijkstra.md": {
    "content": "# Dijkstra's Algorithm\n\n`Dijkstra's algorithm` allows us to find the shortest path between any two vertices of a graph.\n\nIt differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph.\n\n---\n\n### How Dijkstra's Algorithm works\n\nDijkstra's Algorithm works on the basis that any subpath `B -> D` of the shortest path `A -> D` between vertices A and D is also the shortest path between vertices B and D.\n\n![notation](dijkstra0.webp)\n\nDjikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors.\n\nThe algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem.\n\n---\n\n### Example of Dijkstra's algorithm\n\nIt is easier to start with an example and then think about the algorithm.\n\n![notation](dijkstra1.webp)\n\n![notation](dijkstra2.webp)\n\n![notation](dijkstra3.webp)\n\n![notation](dijkstra4.webp)\n\n![notation](dijkstra5.webp)\n\n![notation](dijkstra6.webp)\n\n![notation](dijkstra7.webp)\n\n![notation](dijkstra8.webp)\n\n---\n\n### Djikstra's algorithm pseudocode\n\nWe need to maintain the path distance of every vertex. We can store that in an array of size v, where v is the number of vertices.\n\nWe also want to be able to get the shortest path, not only know the length of the shortest path. For this, we map each vertex to the vertex that last updated its path length.\n\nOnce the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path.\n\nA minimum priority queue can be used to efficiently receive the vertex with least path distance.\n\n    function dijkstra(G, S)\n        for each vertex V in G\n            distance[V] <- infinite\n            previous[V] <- NULL\n            If V != S, add V to Priority Queue Q\n        distance[S] <- 0\n        \n        while Q IS NOT EMPTY\n            U <- Extract MIN from Q\n            for each unvisited neighbour V of U\n                tempDistance <- distance[U] + edge_weight(U, V)\n                if tempDistance < distance[V]\n                    distance[V] <- tempDistance\n                    previous[V] <- U\n        return distance[], previous[]\n\n---\n\n### Implementation\n\n    void Dijkstras();\n    vector<Node*>* AdjacentRemainingNodes(Node* node);\n    Node* ExtractSmallest(vector<Node*>& nodes);\n    int Distance(Node* node1, Node* node2);\n    bool Contains(vector<Node*>& nodes, Node* node);\n\n    vector<Node*> nodes;\n    vector<Edge*> edges;\n\n    class Node {\n    public:\n        Node(char id) : id(id), previous(NULL), distanceFromStart(INT_MAX) { nodes.push_back(this); }\n\n    public:\n        char id;\n        Node* previous;\n        int distanceFromStart;\n    };\n\n    class Edge {\n    public:\n        Edge(Node* node1, Node* node2, int distance) : node1(node1), node2(node2), distance(distance) { edges.push_back(this); }\n        bool Connects(Node* node1, Node* node2) {\n            return (\n            (node1 == this->node1 && node2 == this->node2) ||\n            (node1 == this->node2 && node2 == this->node1));\n        }\n\n    public:\n        Node* node1;\n        Node* node2;\n        int distance;\n    };\n\n    void Dijkstras() {\n        while (nodes.size() > 0) {\n            Node* smallest = ExtractSmallest(nodes);\n            vector<Node*>* adjacentNodes =\n            AdjacentRemainingNodes(smallest);\n\n            const int size = adjacentNodes->size();\n            for (int i = 0; i < size; ++i) {\n                Node* adjacent = adjacentNodes->at(i);\n                int distance = Distance(smallest, adjacent) +\n                        smallest->distanceFromStart;\n\n                if (distance < adjacent->distanceFromStart) {\n                    adjacent->distanceFromStart = distance;\n                    adjacent->previous = smallest;\n                }\n            }\n            delete adjacentNodes;\n        }\n    }\n\n    // Find the node with the smallest distance,\n    // remove it, and return it.\n    Node* ExtractSmallest(vector<Node*>& nodes) {\n        int size = nodes.size();\n        if (size == 0) return NULL;\n        int smallestPosition = 0;\n        Node* smallest = nodes.at(0);\n        for (int i = 1; i < size; ++i) {\n            Node* current = nodes.at(i);\n            if (current->distanceFromStart < smallest->distanceFromStart) {\n                smallest = current;\n                smallestPosition = i;\n            }\n        }\n        nodes.erase(nodes.begin() + smallestPosition);\n        return smallest;\n    }\n\n    // Return all nodes adjacent to 'node' which are still\n    // in the 'nodes' collection.\n    vector<Node*>* AdjacentRemainingNodes(Node* node) {\n        vector<Node*>* adjacentNodes = new vector<Node*>();\n        const int size = edges.size();\n        for (int i = 0; i < size; ++i) {\n            Edge* edge = edges.at(i);\n            Node* adjacent = NULL;\n            if (edge->node1 == node) {\n                adjacent = edge->node2;\n            } else if (edge->node2 == node) {\n                adjacent = edge->node1;\n            }\n            if (adjacent && Contains(nodes, adjacent)) {\n                adjacentNodes->push_back(adjacent);\n            }\n        }\n        return adjacentNodes;\n    }\n\n    // Return distance between two connected nodes\n    int Distance(Node* node1, Node* node2) {\n        const int size = edges.size();\n        for (int i = 0; i < size; ++i) {\n            Edge* edge = edges.at(i);\n            if (edge->Connects(node1, node2)) {\n                return edge->distance;\n            }\n        }\n        return -1;  // should never happen\n    }\n\n    // Does the 'nodes' vector contain 'node'\n    bool Contains(vector<Node*>& nodes, Node* node) {\n        const int size = nodes.size();\n        for (int i = 0; i < size; ++i) {\n            if (node == nodes.at(i)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n---\n\n### Dijkstra's Algorithm Complexity\n\nTime Complexity: `O(E Log V)`\n\nwhere, E is the number of edges and V is the number of vertices.\n\nSpace Complexity: `O(V)`\n\n---\n\n### Dijkstra's Algorithm Applications\n\n* To find the shortest path\n* In social networking applications\n* In a telephone network\n* To find the locations in the map\n",
    "left": "",
    "right": "Dijkstra",
    "name": "dijkstra.md",
    "state": false
  },
  "linked_list.md": {
    "content": "# Linked list Data Structure\n\nA **linked list** is a linear data structure that includes a series of connected nodes. Here, each node stores the **data** and the **address** of the next node. For example,\n\n![notation](linked_list0.webp)\n\nYou have to start somewhere, so we give the address of the first node a special name called `HEAD`. Also, the last node in the linked list can be identified because its next portion points to `NULL`.\n\nLinked lists can be of multiple types: **singly**, **doubly**, and **circular linked list**.\n\n> Note: You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates.\n\n---\n\n### Representation of Linked List\n\nLet's see how each node of the linked list is represented. Each node consists:\n\n* A data item\n* An address of another node\n\nWe wrap both the data item and the next node reference in a struct as:\n\n    struct node\n    {\n        int data;\n        struct node *next;\n    };\n\nUnderstanding the structure of a linked list node is the key to having a grasp on it.\n\nEach struct node has a data item and a pointer to another struct node. Let us create a simple Linked List with three items to understand how this works.\n\n    /* Initialize nodes */\n    struct node *head;\n    struct node *one = NULL;\n    struct node *two = NULL;\n    struct node *three = NULL;\n\n    /* Allocate memory */\n    one = malloc(sizeof(struct node));\n    two = malloc(sizeof(struct node));\n    three = malloc(sizeof(struct node));\n\n    /* Assign data values */\n    one->data = 1;\n    two->data = 2;\n    three->data=3;\n\n    /* Connect nodes */\n    one->next = two;\n    two->next = three;\n    three->next = NULL;\n\n    /* Save address of first node in head */\n    head = one;\n\nIn just a few steps, we have created a simple linked list with three nodes.\n\n![notation](linked_list1.webp)\n\nThe power of a linked list comes from the ability to break the chain and rejoin it. E.g. if you wanted to put an element 4 between 1 and 2, the steps would be:\n\n* Create a new struct node and allocate memory to it.\n* Add its data value as 4\n* Point its next pointer to the struct node containing 2 as the data value\n* Change the next pointer of \"1\" to the node we just created.\n\nDoing something similar in an array would have required shifting the positions of all the subsequent elements.\n\n---\n\n### Linked List Utility\n\nLists are one of the most popular and efficient data structures, with implementation in every programming language like C, C++, Python, Java, and C#.\n\nApart from that, linked lists are a great way to learn how pointers work. By practicing how to manipulate linked lists, you can prepare yourself to learn more advanced data structures like graphs and trees.\n\n---\n\n### Implementation\n\n    struct ListNode {\n        int val;\n        ListNode *next;\n        ListNode() : val(0), next(nullptr) {}\n        ListNode(int x) : val(x), next(nullptr) {}\n        ListNode(int x, ListNode *next) : val(x), next(next) {}\n    };\n\n---\n\n### Linked List Complexity\n\nTime Complexity\n\n||Worst case|Average Case|\n|---|---|---|\n|**Search**| `O(N)`| `O(N)`|\n|**Insert**| `O(1)`| `O(1)`|\n|**Delete**| `O(1)`| `O(1)`|\n\nSpace Complexity: `O(N)`\n\n---\n\n### Linked List Applications\n\n* Dynamic memory allocation\n* Implemented in stack and queue\n* In **undo** functionality of softwares\n* Hash tables, Graphs",
    "left": "Data Structures",
    "right": "Linked list",
    "name": "linked_list.md",
    "state": false
  },
  "stack.md": {
    "content": "# Stack Data Structure\n\nA **stack** is a linear data structure that follows the principle of **Last In First Out (LIFO)**. This means the last element inserted inside the stack is removed first.\n\nYou can think of the stack data structure as the pile of plates on top of another.\n\n![notation](stack0.webp)\n\nHere, you can:\n* Put a new plate on top\n* Remove the top plate\n\nAnd, if you want the plate at the bottom, you must first remove all the plates on top. This is exactly how the stack data structure works.\n\n---\n\n### LIFO Principle of Stack\n\nIn programming terms, putting an item on **top** of the stack is called push and removing an item is called **pop**.\n\n![notation](stack1.webp)\n\nIn the above image, although item **3** was kept last, it was removed first. This is exactly how the **LIFO (Last In First Out) Principle** works.\n\nWe can implement a stack in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.\n\n---\n### Basic Operations of Stack\n\nThere are some basic operations that allow us to perform different actions on a stack.\n\n* **Push:** Add an element to the top of a stack\n* **Pop:** Remove an element from the top of a stack\n* **IsEmpty:** Check if the stack is empty\n* **IsFull:** Check if the stack is full\n* **Peek:** Get the value of the top element without removing it\n\n---\n\n### Working of Stack Data Structure\n\nThe operations work as follows:\n\n1. A pointer called `TOP` is used to keep track of the top element in the stack.\n2. When initializing the stack, we set its value to -1 so that we can check if the stack is empty by comparing `TOP == -1`.\n3. On pushing an element, we increase the value of `TOP` and place the new element in the position pointed to by `TOP`.\n4. On popping an element, we return the element pointed to by `TOP` and reduce its value.\n5. Before pushing, we check if the stack is already full\n6. Before popping, we check if the stack is already empty\n\n![notation](stack2.webp)\n\n---\n\nThe most common stack implementation is using arrays, but it can also be implemented using lists.\n\n    // Creating a stack\n    struct stack {\n    private:\n        vector<int> items;\n        int top, size;\n    public:\n        // Creating a stack with a fixed size\n        stack(int size) {\n            items.resize(size);\n            this->size = size;\n            this->top = -1;\n        }\n\n        // Check if the stack is full\n        bool isfull() {\n            if (this->top == this->size() - 1) return true;\n            else return false;\n        }\n\n        // Check if the stack is empty\n        bool isempty() {\n            if (this->top == -1) return true;\n            else return false;\n        }\n\n        // Add elements into stack\n        void push(int newitem) {\n            if (isfull()) {\n                cout << \"STACK FULL\" << endl;\n            } else {\n                this->top++;\n                this->items[this->top] = newitem;\n            }\n            this->size++;\n        }\n\n        // Remove element from stack\n        void pop() {\n            if (isempty()) {\n                cout << \"\\n STACK EMPTY \\n\";\n            } else {\n                cout << \"Item popped= \" << this->items[this->top];\n                this->top--;\n            }\n            this->size--;\n        }\n    };\n\n---\n\n### Stack Time Complexity\n\nFor the array-based implementation of a stack, the push and pop operations take constant time, i.e. `O(1)`.\n\n---\n\n### Applications of Stack Data Structure\n\nAlthough stack is a simple data structure to implement, it is very powerful. The most common uses of a stack are:\n\n* **To reverse a word** - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.\n\n* **In compilers** - Compilers use the stack to calculate the value of expressions like `2 + 4 / 5 * (7 - 9)` by converting the expression to prefix or postfix form.\n\n* **In browsers** - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.\n",
    "left": "",
    "right": "Stack",
    "name": "stack.md",
    "state": false
  },
  "queue.md": {
    "content": "# Queue Data Structure\n\nA queue is a useful data structure in programming. It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket.\n\n**Queue** follows the **First In First Out (FIFO)** rule - the item that goes in first is the item that comes out first.\n\n![notation](queue0.webp)\n\nIn the above image, since 1 was kept in the queue before 2, it is the first to be removed from the queue as well. It follows the **FIFO** rule.\n\nIn programming terms, putting items in the queue is called **enqueue**, and removing items from the queue is called **dequeue**.\n\nWe can implement the queue in any programming language like C, C++, Java, Python or C#, but the specification is pretty much the same.\n\n---\n### Basic Operations of Queue\n\nA queue is an object (an abstract data structure - ADT) that allows the following operations:\n\n* **Enqueue:** Add an element to the end of the queue\n* **Dequeue:** Remove an element from the front of the queue\n* **IsEmpty:** Check if the queue is empty\n* **IsFull:** Check if the queue is full\n* **Peek:** Get the value of the front of the queue without removing it\n\n---\n\n### Working of Queue\n\nQueue operations work as follows:\n\n* two pointers `FRONT` and `REAR`\n* `FRONT` track the first element of the queue\n* `REAR` track the last element of the queue\n* initially, set value of `FRONT` and `REAR` to -1\n\n### Enqueue Operation\n\n* check if the queue is full\n* for the first element, set the value of `FRONT` to 0\n* increase the `REAR` index by 1\n* add the new element in the position pointed to by `REAR`\n\n### Dequeue Operation\n\n* check if the queue is empty\n* return the value pointed by `FRONT`\n* increase the `FRONT` index by 1\n* for the last element, reset the values of `FRONT` and `REAR` to -1\n\n![notation](queue1.webp)\n\n---\n\n### Implementation\n\nWe usually use arrays to implement queues in Java and C/++. In the case of Python, we use lists.\n\n    struct Queue {\n    private:\n        vector<int> items;\n        int front, rear, size;\n    public:\n        Queue(int size) {\n            items.resize(size);\n            this->front = -1;\n            this->rear = -1;\n            this->size = size;\n        }\n\n        bool isFull() {\n            if (this->front == 0 && this->rear == this->size - 1) return true;\n            return false;\n        }\n\n        bool isEmpty() {\n            if (this->front == -1) return true;\n            else return false;\n        }\n\n        void enQueue(int element) {\n            if (isFull()) {\n                cout << \"Queue is full\";\n            } else {\n                if (this->front == -1) this->front = 0;\n                this->items[++this->rear] = element;\n                cout << \"Inserted \" << element << endl;\n            }\n        }\n\n        int deQueue() {\n            int element;\n            if (isEmpty()) {\n                cout << \"Queue is empty\" << endl;\n                return -1;\n            } else {\n                element = this->items[this->front];\n                if (this->front >= this->rear) {\n                    this->front = -1;\n                    this->rear = -1;\n                } \n                /* Q has only one element, so we reset the queue after deleting it. */\n                else {\n                    this->front++;\n                }\n                cout << \"Deleted \" << element << endl;\n                return element;\n            }\n        }\n    };\n\n---\n\n### Limitations of Queue\n\nAs you can see in the image below, after a bit of enqueuing and dequeuing, the size of the queue has been reduced.\n\n![notation](queue2.webp)\n\nAnd we can only add indexes 0 and 1 only when the queue is reset (when all the elements have been dequeued).\n\nAfter `REAR` reaches the last index, if we can store extra elements in the empty spaces (0 and 1), we can make use of the empty spaces. This is implemented by a modified queue called the [circular queue][1].\n\n---\n\n### Complexity Analysis\n\nThe complexity of enqueue and dequeue operations in a queue using an array is `O(1)`. If you use `pop(N)` in python code, then the complexity might be `O(n)` depending on the position of the item to be popped.\n\n---\n\n### Applications of Queue\n\n* CPU scheduling, Disk Scheduling\n* When data is transferred asynchronously between two processes.The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc\n* Handling of interrupts in real-time systems.\n* Call Center phone systems use Queues to hold people calling them in order.",
    "left": "",
    "right": "Queue",
    "name": "queue.md",
    "state": false
  },
  "priority_queue.md": {
    "content": "# Priority Queue\n\nA **priority queue** is a **special type of queue** in which each element is associated with a **priority value**. And, elements are served on the basis of their priority. That is, higher priority elements are served first.\n\nHowever, if elements with the same priority occur, they are served according to their order in the queue.\n\n**Assigning Priority Value**\n\nGenerally, the value of the element itself is considered for assigning the priority. For example,\n\nThe element with the highest value is considered the highest priority element. However, in other cases, we can assume the element with the lowest value as the highest priority element.\n\nWe can also set priorities according to our needs.\n\n![notation](priority_queue0.webp)\n\n---\n\n### Difference between Priority Queue and Normal Queue\n\nIn a queue, the **first-in-first-out** rule is implemented whereas, in a priority queue, the values are removed **on the basis of priority**. The element with the highest priority is removed first.\n\n---\n\n### Implementation of Priority Queue\n\nPriority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. Among these data structures, heap data structure provides an efficient implementation of priority queues.\n\nHence, we will be using the heap data structure to implement the priority queue in this tutorial. A max-heap is implement is in the following operations. If you want to learn more about it, please visit [max-heap and mean-heap][1].\n\nA comparative analysis of different implementations of priority queue is given below.\n\n|Operations|peek|insert|delete|\n|---|---|---|---|\n|Linked List| `O(1)`| `O(N)`|`O(1)`|\n|Binary Heap| `O(1)`| `O(log(N))`|`O(log(N))`|\n|Binary Search Tree| `O(1)`| `O(log(N))`|`O(log(N))`|\n\n---\n\n### Priority Queue Operations\n\nBasic operations of a priority queue are inserting, removing, and peeking elements.\n\n> Before studying the priority queue, please refer to the [heap data structure][1] for a better understanding of binary heap as it is used to implement the priority queue in this article.\n\n---\n\n**1. Inserting an Element into the Priority Queue**\n\nInserting an element into a priority queue (max-heap) is done by the following steps.\n* Insert the new element at the end of the tree.\n![notation](priority_queue1.webp)\n* Heapify the tree.\n![notation](priority_queue2.webp)\n\nAlgorithm for insertion of an element into priority queue (max-heap)\n\n    if there is no node, \n        create a newNode.\n    else (a node is already present)\n        insert the newNode at the end (last node from left to right.)\n    \n    heapify the array\n\nFor Min Heap, the above algorithm is modified so that `parentNode` is always smaller than `newNode`.\n\n---\n\n**2. Deleting an Element from the Priority Queue**\n\nDeleting an element from a priority queue (max-heap) is done as follows:\n\n* Select the element to be deleted.\n![notation](priority_queue3.webp)\n* Swap it with the last element.\n![notation](priority_queue4.webp)\n* Remove the last element.\n![notation](priority_queue5.webp)\n* Heapify the tree.\n![notation](priority_queue6.webp)\n\nAlgorithm for deletion of an element in the priority queue (max-heap)\n\n    if nodeToBeDeleted is the leafNode\n        remove the node\n    else swap nodeToBeDeleted with the lastLeafNode\n        remove noteToBeDeleted\n    \n    heapify the array\n\nFor Min Heap, the above algorithm is modified so that the both `childNodes` are smaller than `currentNode`.\n\n---\n\n**3. Peeking from the Priority Queue (Find max/min)**\n\nPeek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node.\n\nFor both Max heap and Min Heap\n\n    return rootNode\n\n---\n\n**4. Extract-Max/Min from the Priority Queue**\n\nExtract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it from Min Heap.\n\n---\n\n### Implementation\n\n    struct Heap {\n        vector<int> heapTree;\n        // Function to heapify the tree\n        void heapify(int i) {\n            // Find the largest among root, left child and right child\n            int largest = i;\n            int l = 2 * i + 1;\n            int r = 2 * i + 2;\n            if (l < heapTree.size() && heapTree[l] > heapTree[largest]) largest = l;\n            if (r < heapTree.size() && heapTree[r] > heapTree[largest]) largest = r;\n\n            // Swap and continue heapifying if root is not largest\n            if (largest != i) {\n                swap(heapTree[i], heapTree[largest]);\n                heapify(largest);\n            }\n        }\n\n        // Function to insert an element into the tree\n        void insert(int num) {\n            if (heapTree.size() == 0) heapTree.push_back(num);\n            else {\n                heapTree.push_back(num);\n                for (int i = heapTree.size() / 2 - 1; i >= 0; i--) {\n                    heapify(i);\n                }\n            }\n        }\n\n        // Function to delete an element from the tree\n        void deleteNode(int num) {\n            int i;\n            for (i = 0; i < heapTree.size(); i++) {\n                if (num == heapTree[i]) break;\n            }\n            swap(heapTree[i], heapTree[heapTree.size() - 1]);\n\n            heapTree.pop_back();\n            for (int i = heapTree.size() / 2 - 1; i >= 0; i--) {\n                heapify(i);\n            }\n        }\n    }\n\n---\n\n### Priority Queue Applications\n\nSome of the applications of a priority queue are:\n\n* Dijkstra's algorithm\n* For implementing stack\n* For load balancing and interrupt handling in an operating system\n* For data compression in Huffman code\n",
    "left": "",
    "right": "Priority queue",
    "name": "priority_queue.md",
    "state": false
  },
  "circular_queue.md": {
    "content": "# Circular Queue Data Structure\n\nA circular queue is the extended version of a [regular queue][1] where the last element is connected to the first element. Thus forming a circle-like structure.\n\n![notation](/Users/azamat/WebstormProjects/algomalgo/articles/circular_queue.png)\n\nThe circular queue solves the major limitation of the normal queue. In a normal queue, after a bit of insertion and deletion, there will be non-usable empty space.\n\n![notation](circular_queue1.webp)\n\nHere, indexes **0** and **1** can only be used after resetting the queue (deletion of all elements). This reduces the actual size of the queue.\n\n---\n\n### How Circular Queue Works\n\nCircular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the beginning of the queue.\n\nHere, the circular increment is performed by modulo division with the queue size. That is,\n\n    if REAR + 1 == 5 (overflow!), REAR = (REAR + 1)%5 = 0 (start of queue)\n---\n\n### Circular Queue Operations\n\nThe circular queue work as follows:\n\n* two pointers `FRONT` and `REAR`\n* `FRONT` track the first element of the queue\n* `REAR` track the last elements of the queue\n* initially, set value of `FRONT` and `REAR` to -1\n\n**1. Enqueue Operation**\n\n* check if the queue is full\n* for the first element, set value of `FRONT` to 0\n* circularly increase the `REAR` index by 1 (i.e. if the rear reaches the end, next it would be at the start of the queue)\n* add the new element in the position pointed to by `REAR`\n\n**2. Dequeue Operation**\n\n* check if the queue is empty\n* return the value pointed by `FRONT`\n* circularly increase the `FRONT` index by 1\n* for the last element, reset the values of `FRONT` and `REAR` to -1\n\nHowever, the check for full queue has a new additional case:\n\n* Case 1: `FRONT` = 0 && `REAR == SIZE - 1`\n* Case 2: `FRONT = REAR + 1`\n\nThe second case happens when `REAR` starts from 0 due to circular increment and when its value is just 1 less than `FRONT`, the queue is full.\n\n![notation](circular_queue2.webp)\n\n---\n### Implementation\n\nThe most common queue implementation is using arrays, but it can also be implemented using lists.\n\n    struct Queue {\n        vector<int> items; \n        int front, rear, size;\n\n        Queue(int size) {\n            this->items.resize(size);\n            this->front = -1;\n            this->rear = -1;\n            this->size = size;\n        }\n        // Check if the queue is full\n        bool isFull() {\n            if (this->front == 0 && this->rear == this->size - 1) return true;\n            if (this->front == this->rear + 1) return true;\n            return false;\n        }\n        // Check if the queue is empty\n        bool isEmpty() {\n            if (this->front == -1) return true;\n            else return false;\n        }\n        // Adding an element\n        void enQueue(int element) {\n            if (isFull()) cout << \"Queue is full\";\n            else {\n                if (this->front == -1) this->front = 0;\n                this->rear = (this->rear + 1) % this->size;\n                this->items[this->rear] = element;\n                cout << \"Inserted \" << element << endl;\n            }\n        }\n        // Removing an element\n        int deQueue() {\n            int element;\n            if (isEmpty()) {\n                cout << \"Queue is empty\" << endl;\n                return (-1);\n            } else {\n                element = this->items[this->front];\n                if (this->front == this->rear) {\n                    this->front = -1;\n                    this->rear = -1;\n                }\n                // Q has only one element,\n                // so we reset the queue after deleting it.\n                else this->front = (this->front + 1) % this->size\n                return element;\n            }\n        }\n    };\n---\n\n### Circular Queue Complexity Analysis\n\nThe complexity of the enqueue and dequeue operations of a circular queue is `O(1)` for (array implementations).\n\n---\n\n### Applications of Circular Queue\n\n* CPU scheduling\n* Memory management\n* Traffic Management",
    "left": "",
    "right": "Circular queue",
    "name": "circular_queue.md",
    "state": false
  },
  "hash_table.md": {
    "content": "# Hash Table\n\nThe Hash table data structure stores elements in key-value pairs where\n\n* **Key** - unique integer that is used for indexing the values\n* **Value** - data that are associated with keys\n\n![notation](hash_table0.webp)\n\n---\n\n### Hashing (Hash Function)\n\nIn a hash table, a new index is processed using the keys. And, the element corresponding to that key is stored in the index. This process is called **hashing**.\n\nLet `k` be a key and `h(x)` be a hash function.\n\nHere, `h(k)` will give us a new index to store the element linked with `k`.\n\n![notation](hash_table1.webp)\n\n---\n\n### Hash Collision\n\nWhen the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). This is called a **hash collision**.\n\nWe can resolve the hash collision using one of the following techniques.\n\n* Collision resolution by chaining\n* Open Addressing: Linear/Quadratic Probing and Double Hashing\n\n---\n\n**1. Collision resolution by chaining**\n\nIn chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list.\n\nIf `j` is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, `j` contains `NIL`.\n\n![notation](hash_table2.webp)\n\n**Pseudocode for operations**\n\n    chainedHashSearch(T, k)\n        return T[h(k)]\n    chainedHashInsert(T, x)\n        T[h(x.key)] = x //insert at the head\n    chainedHashDelete(T, x)\n        T[h(x.key)] = NIL\n\n---\n\n**2. Open Addressing**\n\nUnlike chaining, open addressing doesn't store multiple elements into the same slot. Here, each slot is either filled with a single key or left `NIL`.\n\nDifferent techniques used in open addressing are:\n\n**2.1. Linear Probing**\n\nIn linear probing, collision is resolved by checking the next slot.\n\n`h(k, i) = (h′(k) + i) mod m`\n\nwhere\n\n* `i = {0, 1, ….}`\n* `h'(k)` is a new hash function\n\nIf a collision occurs at `h(k, 0)`, then `h(k, 1)` is checked. In this way, the value of `i` is incremented linearly.\n\nThe problem with linear probing is that a cluster of adjacent slots is filled. When inserting a new element, the entire cluster must be traversed. This adds to the time required to perform operations on the hash table.\n\n**2.2 Quadratic Probing**\n\nIt works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation.\n\n`h(k, i) = (h′(k) + c1 * i + c2 * i^2) mod m`\n\nwhere \n\n* `c1` and `c2` are positive auxiliary constants,\n* `i = {0, 1, ….}`\n\n**2.3 Double hashing**\n\nIf a collision occurs after applying a hash function `h(k)`, then another hash function is calculated for finding the next slot.\n\n`h(k, i) = (h1(k) + ih2(k)) mod m`\n\n---\n\n### Good Hash Functions\n\nA good hash function may not prevent the collisions completely however it can reduce the number of collisions.\n\nHere, we will look into different methods to find a good hash function\n\n**1. Division Method**\n\nIf `k` is a key and `m` is the size of the hash table, the hash function `h()` is calculated as:\n\n`h(k) = k mod m`\n\nFor example, If the size of a hash table is `10` and `k = 112` then `h(k) = 112` mod `10 = 2`. The value of `m` must not be the powers of `2`. This is because the powers of `2` in binary format are `10, 100, 1000, …`. When we find `k mod m`, we will always get the lower order p-bits.\n\n    if m = 22, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 01\n    if m = 23, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 001\n    if m = 24, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 0001\n    if m = 2p, then h(k) = p lower bits of m\n\n**2. Multiplication Method**\n\n`h(k) = ⌊m(kA mod 1)⌋`\n\nwhere,\n\n* `kA mod 1` gives the fractional part `kA`,\n* `⌊ ⌋` gives the floor value\n* `A` is any constant. The value of `A` lies between 0 and 1. But, an optimal choice will be `≈ (√5-1)/2` suggested by Knuth.\n\n**3. Universal Hashing**\n\nIn Universal hashing, the hash function is chosen at random independent of keys.\n\n---\n\n### Implementation \n\n    class HashTable {\n        int capacity;\n        list<int> *table;\n\n    public:\n        HashTable(int V);\n        void insertItem(int key, int data);\n        void deleteItem(int key);\n\n        int checkPrime(int n) {\n            int i;\n            if (n == 1 || n == 0) return 0;\n            for (i = 2; i < n / 2; i++) {\n                if (n % i == 0) return 0;\n            }\n            return 1;\n        }\n        int getPrime(int n) {\n            if (n % 2 == 0) n++;\n            while (!checkPrime(n)) n += 2;\n            return n;\n        }\n\n        int hashFunction(int key) {\n            return (key % capacity);\n        }\n        void displayHash();\n    };\n\n        HashTable::HashTable(int c) {\n            int size = getPrime(c);\n            this->capacity = size;\n            table = new list<int>[capacity];\n        }\n        void HashTable::insertItem(int key, int data) {\n            int index = hashFunction(key);\n            table[index].push_back(data);\n        }\n\n        void HashTable::deleteItem(int key) {\n            int index = hashFunction(key);\n\n            list<int>::iterator i;\n            for (i = table[index].begin(); i != table[index].end(); i++) {\n                if (*i == key) break;\n            }\n\n            if (i != table[index].end())\n                table[index].erase(i);\n        }\n\n        void HashTable::displayHash() {\n            for (int i = 0; i < capacity; i++) {\n                cout << \"table[\" << i << \"]\";\n                for (auto x : table[i])\n                    cout << \" --> \" << x;\n                cout << endl;\n            }\n        }\n\n        int main() {\n            int key[] = {231, 321, 212, 321, 433, 262};\n            int data[] = {123, 432, 523, 43, 423, 111};\n            int size = sizeof(key) / sizeof(key[0]);\n\n            HashTable h(size);\n\n            for (int i = 0; i < size; i++)\n                h.insertItem(key[i], data[i]);\n\n            h.deleteItem(12);\n            h.displayHash();\n        }\n\n---\n\n**Applications of Hash Table**\n\nHash tables are implemented where\n\n* constant time lookup and insertion is required\n* cryptographic applications\n* indexing data is required",
    "left": "",
    "right": "Hash table",
    "name": "hash_table.md",
    "state": false
  },
  "bst.md": {
    "content": "# Binary Search Tree (BST)\n\n**Binary search tree** is a data structure that quickly allows us to maintain a sorted list of numbers.\n\n* It is called a binary tree because each tree node has a maximum of two children.\n* It is called a search tree because it can be used to search for the presence of a number in `O(log(N))` time.\n\nThe properties that separate a binary search tree from a regular binary tree is\n\n1. All nodes of left subtree are less than the root node\n2. All nodes of right subtree are more than the root node\n3. Both subtrees of each node are also BSTs i.e. they have the above two properties\n\n![notation](bst0.webp)\n\nThe binary tree on the right isn't a binary search tree because the right subtree of the node \"3\" contains a value smaller than it.\n\nThere are two basic operations that you can perform on a binary search tree:\n\n---\n\n### Search Operation\n\nThe algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root.\n\nIf the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree.\n\n**Algorithm:**\n\n    If root == NULL \n        return NULL;\n    If number == root->data \n        return root->data;\n    If number < root->data \n        return search(root->left)\n    If number > root->data \n        return search(root->right)\n\nLet us try to visualize this with a diagram.\n\n![notation](bst1.webp)\n\n![notation](bst2.webp)\n\n![notation](bst3.webp)\n\n![notation](bst4.webp)\n\nIf the value is found, we return the value so that it gets propagated in each recursion step as shown in the image below.\n\nIf you might have noticed, we have called return search(struct node*) four times. When we return either the new node or NULL, the value gets returned again and again until search(root) returns the final result.\n\n![notation](bst5.webp)\n\nIf the value is not found, we eventually reach the left or right child of a leaf node which is NULL and it gets propagated and returned.\n\n---\n\n### Insert Operation\n\nInserting a value in the correct position is similar to searching because we try to maintain the rule that the left subtree is lesser than root and the right subtree is larger than root.\n\nWe keep going to either right subtree or left subtree depending on the value and when we reach a point left or right subtree is null, we put the new node there.\n\n**Algorithm:**\n\n    If node == NULL \n        return createNode(data)\n    if (data < node->data)\n        node->left = insert(node->left, data);\n    else if (data > node->data)\n        node->right = insert(node->right, data);  \n    return node;\n\nThe algorithm isn't as simple as it looks. Let's try to visualize how we add a number to an existing BST.\n\n![notation](bst6.webp)\n\n![notation](bst7.webp)\n\n![notation](bst8.webp)\n\n![notation](bst9.webp)\n\nWe have attached the node but we still have to exit from the function without doing any damage to the rest of the tree. This is where the `return node;` at the end comes in handy. In the case of `NULL`, the newly created node is returned and attached to the parent node, otherwise the same node is returned without any change as we go up until we return to the root.\n\nThis makes sure that as we move back up the tree, the other node connections aren't changed.\n\n![notation](bst10.webp)\n\n---\n\n### Deletion Operation\n\nThere are three cases for deleting a node from a binary search tree.\n\n**Case I**\n\nIn the first case, the node to be deleted is the leaf node. In such a case, simply delete the node from the tree.\n\n![notation](bst11.webp)\n\n![notation](bst12.webp)\n\n**Case II**\n\nIn the second case, the node to be deleted lies has a single child node. In such a case follow the steps below:\n\n1. Replace that node with its child node.\n2. Remove the child node from its original position.\n\n![notation](bst13.webp)\n\n![notation](bst14.webp)\n\n![notation](bst15.webp)\n\n**Case III**\n\nIn the third case, the node to be deleted has two children. In such a case follow the steps below:\n\n1. Get the inorder successor of that node.\n2. Replace the node with the inorder successor.\n3. Remove the inorder successor from its original position.\n\n![notation](bst16.webp)\n\n![notation](bst17.webp)\n\n![notation](bst18.webp)\n\n---\n\n### Implementation\n\n    struct node {\n      int key;\n      struct node *left, *right;\n    };\n\n    // Create a node\n    struct node *newNode(int item) {\n      struct node *temp = (struct node *)malloc(sizeof(struct node));\n      temp->key = item;\n      temp->left = temp->right = NULL;\n      return temp;\n    }\n\n    // Inorder Traversal\n    void inorder(struct node *root) {\n      if (root != NULL) {\n        // Traverse left\n        inorder(root->left);\n\n        // Traverse root\n        cout << root->key << \" -> \";\n\n        // Traverse right\n        inorder(root->right);\n      }\n    }\n\n    // Insert a node\n    struct node *insert(struct node *node, int key) {\n      // Return a new node if the tree is empty\n      if (node == NULL) return newNode(key);\n\n      // Traverse to the right place and insert the node\n      if (key < node->key)\n        node->left = insert(node->left, key);\n      else\n        node->right = insert(node->right, key);\n\n      return node;\n    }\n\n    // Find the inorder successor\n    struct node *minValueNode(struct node *node) {\n      struct node *current = node;\n\n      // Find the leftmost leaf\n      while (current && current->left != NULL)\n        current = current->left;\n\n      return current;\n    }\n\n    // Deleting a node\n    struct node *deleteNode(struct node *root, int key) {\n      // Return if the tree is empty\n      if (root == NULL) return root;\n\n      // Find the node to be deleted\n      if (key < root->key)\n        root->left = deleteNode(root->left, key);\n      else if (key > root->key)\n        root->right = deleteNode(root->right, key);\n      else {\n        // If the node is with only one child or no child\n        if (root->left == NULL) {\n          struct node *temp = root->right;\n          free(root);\n          return temp;\n        } else if (root->right == NULL) {\n          struct node *temp = root->left;\n          free(root);\n          return temp;\n        }\n\n        // If the node has two children\n        struct node *temp = minValueNode(root->right);\n\n        // Place the inorder successor in position of the node to be deleted\n        root->key = temp->key;\n\n        // Delete the inorder successor\n        root->right = deleteNode(root->right, temp->key);\n      }\n      return root;\n    }\n\n---\n\n### Binary Search Tree Complexities\n\n**Time Complexity**\n\n|Operation|Best Case Complexity|Average Case Complexity|Worst Case Complexity|\n|---|---|---|---|\n|Search|`O(log(N))`|`O(log(N))`|`O(N)`|\n|Insertion|`O(log(N))`| `O(log(N))`|`O(N)`|\n|Deletion|`O(log(N))`| `O(log(N))`|`O(N)`|\n\nHere, `n` is the number of nodes in the tree.\n\n**Space Complexity**\n\nThe space complexity for all the operations is `O(N)`.\n\n---\n\n### Binary Search Tree Applications\n\n1. In multilevel indexing in the database\n2. For dynamic sorting\n3. For managing virtual memory areas in Unix kernel\n",
    "left": "",
    "right": "Bst",
    "name": "bst.md",
    "state": false
  }
}